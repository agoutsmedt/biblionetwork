#'
#' This function uses data.table package and is thus very fast. It allows the user to compute the coupling angle
#' on a very large dataframe very quickly.
#'
#' This function is a relatively general function that can also be used
#' 1) for title co-occurence networks (taking care of
#' the lenght of the title thanks to the coupling angle measure);
#' 2) for co-authorship networks (taking care of the
#' number of co-authors an author has collaborated with on a period)
#'
#' @param dt
#' For bibliographic coupling, the dataframe with citing and cited documents. It could also be used
#' 1) for title co-occurence network, with `source` being the articles,
#' and `ref` being the list of words in articles titles;
#' 2) for co-authorship network,
#' with `source` being the authors, and `ref` the list of articles. For co-authorship,
#' rather use the `coauth_network` function.
#'
#' @param source
#' the column name of the source identifiers, that is the documents that are citing.
#'
#' @param ref
#'the column name of the cited references identifiers.
#'
#' @param normalized_weight_only
#' if set to FALSE, the function returns the weights normalized by the cosine measure,
#' but also simply the number of shared references.
#'
#' @param weight_threshold
#' Correspond to the value of the non-normalized weights of edges. The function just keeps the edges
#' that have a non-normalized weight superior to the `weight_threshold`. In a large bibliographic coupling network,
#' you can consider for instance that sharing only one reference is not sufficient/significant for two articles to be linked together.
#' This parameter could also be modified to avoid creating untractable networks with too many edges.
#'
#' @param output_in_character
#' If TRUE, the function ends by transforming the `from` and `to` columns in character, to make the
#' creation of a tidygraph object easier.
#'
#' @return A data.table with the articles (or authors) identifier in `from` and `to` columns, with one or two additional columns (the coupling angle measure and
#' the number of shared references). It also keeps a copy of `from` and `to` in the `Source` and `Target` columns. This is useful is you
#' are using the tidygraph package then, where `from` and `to` values are modified when creating a graph.
#'
#' @references Sen, S. K., & Gan, S. K. (1983). A mathematical extension of the idea of bibliographic coupling and its applications.
#'
#' @export
# Making sure the table is a datatable
dt <- data.table::data.table(dt)
# Renaming and simplifying
data.table::setnames(dt, c(source,ref), c("id_art", "id_ref"))
dt <- dt[,list(id_art,id_ref)]
data.table::setkey(dt,id_ref,id_art)
# removing duplicated citations with exactly the same source and target
dt <- unique(dt)
# remove loop
dt <- dt[id_art!=id_ref]
# Removing references cited only once:
dt <- dt[,N := .N, by = id_ref][N > 1][, list(id_art,id_ref)]
# Computing how many items each citing document has (necessary for normalization later)
id_nb_cit <-  dt[,list(nb_cit = .N),by=id_art]
#Creating every combinaison of articles per references
bib_coup <- dt[,list(Target = rep(id_art[1:(length(id_art)-1)],(length(id_art)-1):1),
Source = rev(id_art)[sequence((length(id_art)-1):1)]),
by= id_ref]
# remove loop
bib_coup <- bib_coup[Source!=Target]
#Calculating the weight
bib_coup <- bib_coup[,.N,by=list(Target,Source)] # This is the number of go references
# keeping edges over threshold
bib_coup <- bib_coup[N>=weight_threshold]
# We than do manipulations to normalize this number with the cosine measure
bib_coup <-  merge(bib_coup, id_nb_cit, by.x = "Target",by.y = "id_art" )
data.table::setnames(bib_coup,"nb_cit", "nb_cit_Target")
bib_coup <-  merge(bib_coup, id_nb_cit, by.x = "Source",by.y = "id_art" )
data.table::setnames(bib_coup,"nb_cit", "nb_cit_Source")
bib_coup[,weight := N/sqrt(nb_cit_Target*nb_cit_Source)] # cosine measure
# Renaming columns
data.table::setnames(bib_coup, c("N"),
c("nb_shared_references"))
# Transforming the Source and Target columns in character (and keeping the Source and Target in copy)
# Then selection which columns to return
if(output_in_character == TRUE){
bib_coup$from <- as.character(bib_coup$Source)
bib_coup$to <- as.character(bib_coup$Target)
if(normalized_weight_only==TRUE){
return (bib_coup[, c("from","to","weight","Source","Target")])
} else {
return (bib_coup[, c("from","to","weight","nb_shared_references","Source","Target")])
}
}
else{
if(normalized_weight_only==TRUE){
return (bib_coup[, c("Source","Target","weight")])
} else {
return (bib_coup[, c("Source","Target","weight","nb_shared_references")])
}
}
}
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
library(devtools)
load_all()
x = readRDS(paste0(getwd(),"/data/Ref_stagflation.RDS"))
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
1
eval
abort
restard
rrestart
restart
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
library(devtools)
load_all()
x = readRDS(paste0(getwd(),"/data/Ref_stagflation.RDS"))
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
undebug(logit)
undebug()
undebug(biblio_coupling)
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
debug(ls)
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
a
q
undebug(ls)
undebug(ls)
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
undebug(biblio_coupling)
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
library(devtools)
load_all()
check()
load_all()
check()
load_all()
load_all()
load_all()
document()
check()
load_all()
load_all()
check()
load_all()
document()
load_all()
check()
x = readRDS(paste0(getwd(),"/data/Ref_stagflation.RDS"))
biblio_coupling(x, "Citing_ItemID_Ref","ItemID_Ref")
use_mit_licence
use_mit_license()
check()
usethis::use_data(x)
Ref_stagflation = readRDS(paste0(getwd(),"/data/Ref_stagflation.RDS"))
Nodes_stagflation = readRDS(paste0(getwd(),"/data/Nodes_stagflation.RDS"))
usethis::use_data(Ref_stagflation)
usethis::use_data(Nodes_stagflation)
load("~/MEGA/Research/R/Packages/biblionetwork/data/Nodes_stagflation.rda")
View(Nodes_stagflation)
load("~/MEGA/Research/R/Packages/biblionetwork/data/Nodes_stagflation.rda")
View(Nodes_stagflation)
check()
View(Nodes_stagflation)
load("~/MEGA/Research/R/Packages/biblionetwork/data/Ref_stagflation.rda")
View(Ref_stagflation)
check()
document()
document()
document()
load_all()
check()
load_all()
check()
load_all()
check()
load_all()
load_all()
load_all()
load_(all)
load_all()
load_all()
check()
install()
install.packages("Rtools")
library(biblionetwork)
load("~/MEGA/Research/R/Packages/biblionetwork/data/Ref_stagflation.rda")
biblio_coupling(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
library(devtools)
use_readme_rmd()
library(biblionetwork)
Ref_stagflation <- data("Ref_stagflation")
Ref_stagflation
Ref_stagflation
library(biblionetwork)
Ref_stagflation <- data("Ref_stagflation.rda")
library(biblionetwork)
Ref_stagflation <- data("~\data\Ref_stagflation.rda")
getwd()
library(biblionetwork)
Ref_stagflation <- data("~/data/Ref_stagflation.rda")
library(biblionetwork)
Ref_stagflation <- data("bibllionetwork/data/Ref_stagflation.rda")
library(biblionetwork)
Ref_stagflation <- data("/biblionetwork/data/Ref_stagflation.rda")
library(biblionetwork)
Ref_stagflation <- data("~/biblionetwork/data/Ref_stagflation.rda")
data(Ref_stagflation)
Ref_stagflation
library(biblionetwork)
data(Ref_stagflation)
library(biblionetwork)
Ref_stagflation <- data(Ref_stagflation)
edges <- biblio_coupling(Ref_stagflation, source = "Citing_ItemID_Ref", ref = "ItemID_Ref", normalized_weight_only = FALSE)
Ref_stagflation
data(Ref_stagflation)
data(Ref_stagflation)
data(Ref_stagflation)
Ref_stagflation
library(biblionetwork)
#data(Ref_stagflation)
#Ref_stagflation <- data(Ref_stagflation)
edges <- biblio_coupling(Ref_stagflation, source = "Citing_ItemID_Ref", ref = "ItemID_Ref", normalized_weight_only = FALSE)
edges
library(biblionetwork)
#data(Ref_stagflation)
#Ref_stagflation <- data(Ref_stagflation)
edges <- biblio_coupling(Ref_stagflation, source = "Citing_ItemID_Ref", ref = "ItemID_Ref", normalized_weight_only = FALSE)
edges
devtools::build_readme()
devtools::build_readme()
document()
document()
install.packages("Rdpack")
load_all()
rlang::last_error()
load_all()
document()
load_all()
library(Rdpack)
load_all()
load_all()
document()
?biblio_coupling
check()
document()
check()
document()
check()
document()
check()
check()
load_all()
?biblio_coupling
document()
load_all()
?biblio_coupling
?Rdpack::insertRef
document()
?biblio_coupling
?Rdpack::insertRef
document()
check()
document()
document()
load_all()
?biblio_coupling
install()
?biblio_coupling
library(biblionetwork)
?biblio_coupling
biblio_coupling
?biblio_coupling
help(biblio_coupling)
library(biblionetwork)
edges <- biblio_coupling(Ref_stagflation, source = "Citing_ItemID_Ref", ref = "ItemID_Ref", normalized_weight_only = FALSE)
?biblio_coupling
library(devtools)
check()
load_all()
load("~/MEGA/Research/R/Packages/biblionetwork/data/Ref_stagflation.rda")
coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
?if
?if()
?if()
load_all()
coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
load_all()
coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
x = coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
x
coupling_strength <- function(dt, source, ref, weight_threshold = 1, output_in_character = FALSE)
{
#' function for edges of bibliographic coupling
#'
#' @description This function calculates the coupling strength measure \insertCite{@Following @vladutz1984 and @shen2019}{biblionetwork}
#' from a direct citation data frame. It is a refinement of [biblio_coupling()][biblionetwork::biblio_coupling()]:
#' it takes into account the frequency with which a reference shared by two articles has been cited in the whole corpus.
#' In other words, the most cited references are less important in the links between two articles, than references that have
#' been rarely cited. To a certain extent, it is similar to the TF-IDF measure.
#'
#' @param dt
#' The table with citing and cited documents.
#'
#' @param source
#' the column name of the source identifiers, that is the documents that are citing.
#'
#' @param ref
#'the column name of the references that are cited.
#'
#' @param weight_threshold
#' Correspond to the value of the non-normalized weights of edges. The function just keeps the edges
#' that have a non-normalized weight superior to the `weight_threshold`. In a large bibliographic coupling network,
#' you can consider for instance that sharing only one reference is not sufficient/significant for two articles to be linked together.
#' This parameter could also be modified to avoid creating untractable networks with too many edges.
#'
#' @param output_in_character
#' If TRUE, the function ends by transforming the `from` and `to` columns in character, to make the
#' creation of a [tidygraph](https://tidygraph.data-imaginist.com/index.html) graph easier.
#'
#' @return A data.table with the articles identifiers in `from` and `to` columns, with the coupling strength measure in
#' another column. It also keeps a copy of `from` and `to` in the `Source` and `Target` columns. This is useful is you
#' are using the tidygraph package then, where `from` and `to` values are modified when creating a graph.
#'
#' @references
#' \insertAllCited{}
#'
#' @export
#' @import data.table
#' @import Rdpack
# Listing the variables not in the global environment to avoid a "note" saying "no visible binding for global variable ..." when using check()
# See https://www.r-bloggers.com/2019/08/no-visible-binding-for-global-variable/
id_ref <- id_art <- N <- Source <- Target <- weight <- nb_cit <- . <- nb_ref_Target <- nb_ref_Source <- NULL
# Making sure the table is a datatable
dt <- data.table::data.table(dt)
# Renaming and simplifying
data.table::setnames(dt, c(source,ref), c("id_art", "id_ref"))
dt <- dt[,c("id_art","id_ref")]
data.table::setkey(dt,id_ref,id_art)
# removing duplicated citations with exactly the same source and target
dt <- unique(dt)
# remove loop
dt <- dt[id_art!=id_ref]
# Computing how many items each citing document has (necessary for normalization later)
id_nb_ref <-  dt[,list(nb_ref = .N),by=id_art]
# Removing references cited only once:
dt <- dt[,N := .N, by = id_ref][N > 1][, list(id_art,id_ref)]
# Computing how many times each reference is cited
ref_nb_cit <-  dt[,list(nb_cit = .N),by=id_ref]
# Computing the total number of documents in the corpus.
nb_doc <-  dt[unique(id_art)][,list(n_document = .N)]
# Creating every combinaison of articles per references
bib_coup <- dt[,list(Target = rep(id_art[1:(length(id_art)-1)],(length(id_art)-1):1),
Source = rev(id_art)[sequence((length(id_art)-1):1)]),
by= id_ref]
# remove loop
bib_coup <- bib_coup[Source!=Target]
# Inverse Source and Target so that couple of Source/Target are always on the same side
bib_coup <- bib_coup[Source > Target, c("Target", "Source") := list(Source, Target)] # exchanging
###### Add columns with info for weighting
#Calculating the number of references in common and deleting the links between articles that share less than weight_threshold
bib_coup <- bib_coup[,N:= .N,by=list(Target,Source)][N>=weight_threshold]
# integrating the number of documents
bib_coup[,nb_doc:=nb_doc]
# merge the number of occurence of a ref in a document
bib_coup <-  merge(bib_coup, ref_nb_cit, by = "id_ref")
# merge the lenght of reference list
bib_coup <-  merge(bib_coup, id_nb_ref, by.x = "Target",by.y = "id_art" )
setnames(bib_coup,"nb_ref", "nb_ref_Target")
bib_coup <-  merge(bib_coup, id_nb_ref, by.x = "Source",by.y = "id_art" )
setnames(bib_coup,"nb_ref", "nb_ref_Source")
# CS
bib_coup[,weight := (sum(log(nb_doc/nb_cit))) / (nb_ref_Target*nb_ref_Source), .(Source,Target)]
# Keep only unique couple
#bib_coup <- bib_coup[, head(.SD, 1), .(Source,Target)]
# copying the Source and Target columns in case of using Tidygraph later
bib_coup[, `:=` (from = Source, to = Target)]
#Transforming in character
if(output_in_character == TRUE){
bib_coup$from <- as.character(bib_coup$Source)
bib_coup$to <- as.character(bib_coup$Target)
}
# return (bib_coup)
}
coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
x = coupling_strength(Ref_stagflation, "Citing_ItemID_Ref","ItemID_Ref")
x
# Making sure the table is a datatable
dt <- data.table::data.table(Ref_stagflation)
dt
source="Citing_ItemID_Ref"
ref="ItemID_Ref"
# Renaming and simplifying
data.table::setnames(dt, c(source,ref), c("id_art", "id_ref"))
dt <- dt[,c("id_art","id_ref")]
data.table::setkey(dt,id_ref,id_art)
dt
# removing duplicated citations with exactly the same source and target
dt <- unique(dt)
# remove loop
dt <- dt[id_art!=id_ref]
# Computing how many items each citing document has (necessary for normalization later)
id_nb_ref <-  dt[,list(nb_ref = .N),by=id_art]
# Removing references cited only once:
dt <- dt[,N := .N, by = id_ref][N > 1][, list(id_art,id_ref)]
# Computing how many times each reference is cited
ref_nb_cit <-  dt[,list(nb_cit = .N),by=id_ref]
# Computing the total number of documents in the corpus.
nb_doc <-  dt[unique(id_art)][,list(n_document = .N)]
# Creating every combinaison of articles per references
bib_coup <- dt[,list(Target = rep(id_art[1:(length(id_art)-1)],(length(id_art)-1):1),
Source = rev(id_art)[sequence((length(id_art)-1):1)]),
by= id_ref]
# remove loop
bib_coup <- bib_coup[Source!=Target]
# Inverse Source and Target so that couple of Source/Target are always on the same side
bib_coup <- bib_coup[Source > Target, c("Target", "Source") := list(Source, Target)] # exchanging
bib_coup
bib_coup
weight_threshold = 1
###### Add columns with info for weighting
#Calculating the number of references in common and deleting the links between articles that share less than weight_threshold
bib_coup <- bib_coup[,N:= .N,by=list(Target,Source)][N>=weight_threshold]
# integrating the number of documents
bib_coup[,nb_doc:=nb_doc]
# merge the number of occurence of a ref in a document
bib_coup <-  merge(bib_coup, ref_nb_cit, by = "id_ref")
# merge the lenght of reference list
bib_coup <-  merge(bib_coup, id_nb_ref, by.x = "Target",by.y = "id_art" )
setnames(bib_coup,"nb_ref", "nb_ref_Target")
bib_coup <-  merge(bib_coup, id_nb_ref, by.x = "Source",by.y = "id_art" )
setnames(bib_coup,"nb_ref", "nb_ref_Source")
bib_coup
# CS
bib_coup[,weight := (sum(log(nb_doc/nb_cit))) / (nb_ref_Target*nb_ref_Source), .(Source,Target)]
bib_coup
# copying the Source and Target columns in case of using Tidygraph later
bib_coup[, `:=` (from = Source, to = Target)]
bib_coup
output_in_character = TRUE
#Transforming in character
if(output_in_character == TRUE){
bib_coup$from <- as.character(bib_coup$from)
bib_coup$to <- as.character(bib_coup$to)
}
bib_coup
return (bib_coup[, c("from","to","weight","Source","Target")])
bib_coup[, c("from","to","weight","Source","Target")]
# Listing the variables not in the global environment to avoid a "note" saying "no visible binding for global variable ..." when using check()
# See https://www.r-bloggers.com/2019/08/no-visible-binding-for-global-variable/
id_ref <- id_art <- N <- Source <- Target <- weight <- nb_cit <- . <- nb_ref_Target <- nb_ref_Source <- NULL
# Making sure the table is a datatable
dt <- data.table::data.table(Ref_stagflation)
dt
# Renaming and simplifying
data.table::setnames(dt, c(source,ref), c("id_art", "id_ref"))
dt <- dt[,c("id_art","id_ref")]
data.table::setkey(dt,id_ref,id_art)
# removing duplicated citations with exactly the same source and target
dt <- unique(dt)
# remove loop
dt <- dt[id_art!=id_ref]
# Computing how many items each citing document has (necessary for normalization later)
id_nb_ref <-  dt[,list(nb_ref = .N),by=id_art]
# Removing references cited only once:
dt <- dt[,N := .N, by = id_ref][N > 1][, list(id_art,id_ref)]
# Computing how many times each reference is cited
ref_nb_cit <-  dt[,list(nb_cit = .N),by=id_ref]
# Computing the total number of documents in the corpus.
nb_doc <-  dt[unique(id_art)][,list(n_document = .N)]
# Creating every combinaison of articles per references
bib_coup <- dt[,list(Target = rep(id_art[1:(length(id_art)-1)],(length(id_art)-1):1),
Source = rev(id_art)[sequence((length(id_art)-1):1)]),
by= id_ref]
# remove loop
bib_coup <- bib_coup[Source!=Target]
# Inverse Source and Target so that couple of Source/Target are always on the same side
bib_coup <- bib_coup[Source > Target, c("Target", "Source") := list(Source, Target)] # exchanging
###### Add columns with info for weighting
#Calculating the number of references in common and deleting the links between articles that share less than weight_threshold
bib_coup <- bib_coup[,N:= .N,by=list(Target,Source)][N>=weight_threshold]
# integrating the number of documents
bib_coup[,nb_doc:=nb_doc]
# merge the number of occurence of a ref in a document
bib_coup <-  merge(bib_coup, ref_nb_cit, by = "id_ref")
# merge the lenght of reference list
bib_coup <-  merge(bib_coup, id_nb_ref, by.x = "Target",by.y = "id_art" )
setnames(bib_coup,"nb_ref", "nb_ref_Target")
bib_coup <-  merge(bib_coup, id_nb_ref, by.x = "Source",by.y = "id_art" )
setnames(bib_coup,"nb_ref", "nb_ref_Source")
# CS
bib_coup[,weight := (sum(log(nb_doc/nb_cit))) / (nb_ref_Target*nb_ref_Source), .(Source,Target)]
# copying the Source and Target columns in case of using Tidygraph later
bib_coup[, `:=` (from = Source, to = Target)]
#Transforming in character
if(output_in_character == TRUE){
bib_coup$from <- as.character(bib_coup$from)
bib_coup$to <- as.character(bib_coup$to)
}
bib_coup[, c("from","to","weight","Source","Target")]
load_all()
